{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "351bb721",
      "metadata": {
        "id": "351bb721"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The purpose of this project is to inspect, formulate a model to visualise data and develop machine learning algorithms using data science tools in Python. This project is created to test on:\n",
        "\n",
        "\n",
        "- Understanding data using basic statistical opertations and Natural Language Processing Tools\n",
        "- Separate features and labels in a data set\n",
        "- Normalise the data set\n",
        "- Split the data into testing and training data\n",
        "- Using SVM model to perform multi-class classification\n",
        "- Predicting the testing data and communicating analysis using Quadratic Weighted Kappa\n",
        "- Providing a concise conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure of content\n",
        "\n",
        "1. Read and Describe contents of file\n",
        "2. Supervised Machine Learning\n",
        "3. Feature, Label Separation\n",
        "4. Train | Test Data Split\n",
        "5. Binary vs Multi-class Model\n",
        "6. Data Normalisation\n",
        "7. Feature Column Selection\n",
        "8. Support Vector Machine\n",
        "9. Building a predictive model\n",
        "10. Predicting from the model built\n",
        "11. Confusion Matrix\n",
        "12. QWK Score\n",
        "13. Kaggle Submission Prediction"
      ],
      "metadata": {
        "id": "k3FU3H9mnmaC"
      },
      "id": "k3FU3H9mnmaC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Description and Exploration\n",
        "\n",
        "The aim of this project is to utilize Python as a data science tool to investigate and visualize data, as well as build machine learning models. The project will involve analyzing and creating an intelligent system that can classify individuals into credit score ranges based on a combination of banking detaiks and credit-related data that has been gathered over time. Assuming myself as the data scientist's of this company, I am entitled to analyse and preprocess the data before creating machine learning models with the right algorithms to forecast each customer's credit score. As a result of automating this procedure, the system can lessen the manual labour needed to go through enormous volumes of data. At teh same time this also offers a more precise and effective way to determine credit scores"
      ],
      "metadata": {
        "id": "J8OOCgiXn_La"
      },
      "id": "J8OOCgiXn_La"
    },
    {
      "cell_type": "markdown",
      "id": "0fcf2f83",
      "metadata": {
        "id": "0fcf2f83"
      },
      "source": [
        "### Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5beecdd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "collapsed": true,
        "id": "5beecdd2",
        "outputId": "bb622abf-78de-4e69-82d6-0381260d8726"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Credit_Scores_Dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0b5d918dc1d8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# reading the dataset CSV file and creates a pandas DataFrame object called 'dataset' containing the data from the CSV file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Credit_Scores_Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Credit_Scores_Dataset.csv'"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# reading the dataset CSV file and creates a pandas DataFrame object called 'dataset' containing the data from the CSV file.\n",
        "dataset = pd.read_csv('Credit_Scores_Dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bfc58f",
      "metadata": {
        "id": "83bfc58f"
      },
      "source": [
        "### Basic Descriptive Statistics of Data\n",
        "\n",
        "The dataset contains the basic bank details and the credit-related information of the customers of a global finance company.\n",
        "\n",
        "The examples below show some basic descriptive statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65748fd2",
      "metadata": {
        "id": "65748fd2",
        "outputId": "97dd8ba8-06aa-46e4-a0d2-2dc612bc3915"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Month</th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Annual_Income</th>\n",
              "      <th>Monthly_Inhand_Salary</th>\n",
              "      <th>Num_Bank_Accounts</th>\n",
              "      <th>Num_Credit_Card</th>\n",
              "      <th>Interest_Rate</th>\n",
              "      <th>Num_of_Loan</th>\n",
              "      <th>...</th>\n",
              "      <th>Credit_Mix</th>\n",
              "      <th>Outstanding_Debt</th>\n",
              "      <th>Credit_Utilization_Ratio</th>\n",
              "      <th>Credit_History_Age</th>\n",
              "      <th>Payment_of_Min_Amount</th>\n",
              "      <th>Total_EMI_per_month</th>\n",
              "      <th>Amount_invested_monthly</th>\n",
              "      <th>Payment_Behaviour</th>\n",
              "      <th>Monthly_Balance</th>\n",
              "      <th>Credit_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>106981</td>\n",
              "      <td>8</td>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "      <td>14619.585</td>\n",
              "      <td>1005.298750</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>125.33</td>\n",
              "      <td>38.883189</td>\n",
              "      <td>222</td>\n",
              "      <td>1</td>\n",
              "      <td>11.620889</td>\n",
              "      <td>32.846250</td>\n",
              "      <td>202</td>\n",
              "      <td>279.724565</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>108774</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>12</td>\n",
              "      <td>70883.440</td>\n",
              "      <td>5663.953333</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>604.77</td>\n",
              "      <td>31.131854</td>\n",
              "      <td>356</td>\n",
              "      <td>0</td>\n",
              "      <td>97.133997</td>\n",
              "      <td>39.858686</td>\n",
              "      <td>201</td>\n",
              "      <td>526.033197</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>111896</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "      <td>12</td>\n",
              "      <td>14395.830</td>\n",
              "      <td>1027.652500</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>28</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2841.00</td>\n",
              "      <td>37.587389</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>74.795382</td>\n",
              "      <td>31.947738</td>\n",
              "      <td>201</td>\n",
              "      <td>258.713002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32731</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>11189.065</td>\n",
              "      <td>1159.422083</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>761.18</td>\n",
              "      <td>33.980973</td>\n",
              "      <td>126</td>\n",
              "      <td>0</td>\n",
              "      <td>18.439801</td>\n",
              "      <td>16.806258</td>\n",
              "      <td>201</td>\n",
              "      <td>324.284100</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128760</td>\n",
              "      <td>7</td>\n",
              "      <td>37</td>\n",
              "      <td>3</td>\n",
              "      <td>78956.730</td>\n",
              "      <td>6523.727500</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>436.82</td>\n",
              "      <td>27.684657</td>\n",
              "      <td>265</td>\n",
              "      <td>1</td>\n",
              "      <td>128.558654</td>\n",
              "      <td>70.788144</td>\n",
              "      <td>102</td>\n",
              "      <td>669.025667</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>151390</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>11</td>\n",
              "      <td>21167.555</td>\n",
              "      <td>1829.962917</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1286.57</td>\n",
              "      <td>33.708627</td>\n",
              "      <td>361</td>\n",
              "      <td>0</td>\n",
              "      <td>40.335282</td>\n",
              "      <td>22.819491</td>\n",
              "      <td>202</td>\n",
              "      <td>277.370503</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>69108</td>\n",
              "      <td>7</td>\n",
              "      <td>43</td>\n",
              "      <td>14</td>\n",
              "      <td>44964.220</td>\n",
              "      <td>3898.018333</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>210.15</td>\n",
              "      <td>28.666651</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>75.407970</td>\n",
              "      <td>46.850154</td>\n",
              "      <td>201</td>\n",
              "      <td>306.094503</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>26275</td>\n",
              "      <td>7</td>\n",
              "      <td>50</td>\n",
              "      <td>13</td>\n",
              "      <td>140390.320</td>\n",
              "      <td>11888.193330</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1423.23</td>\n",
              "      <td>32.966273</td>\n",
              "      <td>374</td>\n",
              "      <td>0</td>\n",
              "      <td>182.160424</td>\n",
              "      <td>133.213034</td>\n",
              "      <td>103</td>\n",
              "      <td>1020.195699</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>139554</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>6</td>\n",
              "      <td>54284.940</td>\n",
              "      <td>4673.745000</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>30</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>4237.27</td>\n",
              "      <td>38.040801</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "      <td>190.641975</td>\n",
              "      <td>73.850532</td>\n",
              "      <td>101</td>\n",
              "      <td>392.593676</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>99702</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>3</td>\n",
              "      <td>19375.760</td>\n",
              "      <td>1633.646667</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1053.72</td>\n",
              "      <td>31.459343</td>\n",
              "      <td>325</td>\n",
              "      <td>1</td>\n",
              "      <td>10.511619</td>\n",
              "      <td>26.628272</td>\n",
              "      <td>103</td>\n",
              "      <td>335.390534</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID  Month  Age  Occupation  Annual_Income  Monthly_Inhand_Salary  \\\n",
              "0  106981      8   41           2      14619.585            1005.298750   \n",
              "1  108774      1   28          12      70883.440            5663.953333   \n",
              "2  111896      3   29          12      14395.830            1027.652500   \n",
              "3   32731      2   25           1      11189.065            1159.422083   \n",
              "4  128760      7   37           3      78956.730            6523.727500   \n",
              "5  151390      5   50          11      21167.555            1829.962917   \n",
              "6   69108      7   43          14      44964.220            3898.018333   \n",
              "7   26275      7   50          13     140390.320           11888.193330   \n",
              "8  139554      1   29           6      54284.940            4673.745000   \n",
              "9   99702      1   18           3      19375.760            1633.646667   \n",
              "\n",
              "   Num_Bank_Accounts  Num_Credit_Card  Interest_Rate  Num_of_Loan  ...  \\\n",
              "0                  7                7             19            1  ...   \n",
              "1                  4                4             10            3  ...   \n",
              "2                  8                8             28            7  ...   \n",
              "3                  6                3             15            3  ...   \n",
              "4                  7                3             14            2  ...   \n",
              "5                  2                7              9            3  ...   \n",
              "6                  6                7             15            3  ...   \n",
              "7                  5                2              4            3  ...   \n",
              "8                  7                8             30            7  ...   \n",
              "9                  4                3              6            1  ...   \n",
              "\n",
              "   Credit_Mix  Outstanding_Debt  Credit_Utilization_Ratio  Credit_History_Age  \\\n",
              "0           2            125.33                 38.883189                 222   \n",
              "1           2            604.77                 31.131854                 356   \n",
              "2           1           2841.00                 37.587389                  27   \n",
              "3           2            761.18                 33.980973                 126   \n",
              "4           2            436.82                 27.684657                 265   \n",
              "5           3           1286.57                 33.708627                 361   \n",
              "6           2            210.15                 28.666651                 400   \n",
              "7           3           1423.23                 32.966273                 374   \n",
              "8           1           4237.27                 38.040801                  59   \n",
              "9           2           1053.72                 31.459343                 325   \n",
              "\n",
              "   Payment_of_Min_Amount  Total_EMI_per_month  Amount_invested_monthly  \\\n",
              "0                      1            11.620889                32.846250   \n",
              "1                      0            97.133997                39.858686   \n",
              "2                      1            74.795382                31.947738   \n",
              "3                      0            18.439801                16.806258   \n",
              "4                      1           128.558654                70.788144   \n",
              "5                      0            40.335282                22.819491   \n",
              "6                      0            75.407970                46.850154   \n",
              "7                      0           182.160424               133.213034   \n",
              "8                      1           190.641975                73.850532   \n",
              "9                      1            10.511619                26.628272   \n",
              "\n",
              "   Payment_Behaviour  Monthly_Balance  Credit_Score  \n",
              "0                202       279.724565             2  \n",
              "1                201       526.033197             2  \n",
              "2                201       258.713002             1  \n",
              "3                201       324.284100             2  \n",
              "4                102       669.025667             2  \n",
              "5                202       277.370503             1  \n",
              "6                201       306.094503             2  \n",
              "7                103      1020.195699             3  \n",
              "8                101       392.593676             2  \n",
              "9                103       335.390534             2  \n",
              "\n",
              "[10 rows x 24 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The head() function in pandas library gets the first 5 rows of data from the csv file. This allows us to get a basic idea on the columns in the data set and the values for each columns in the data set\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73269ba6",
      "metadata": {
        "collapsed": true,
        "id": "73269ba6",
        "outputId": "3a5a9784-d76b-427c-8061-8a5968dfda8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2100, 24)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape attribute allows us to understand the number of rows and columns in the dataset. From this dataset, we can see the data set has 2100 rows and 24 columns"
      ],
      "metadata": {
        "id": "BF21hv7bqDrA"
      },
      "id": "BF21hv7bqDrA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "551e56be",
      "metadata": {
        "collapsed": true,
        "id": "551e56be",
        "outputId": "ef29b70d-3295-48f0-b144-240342952964"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID                            int64\n",
              "Month                         int64\n",
              "Age                           int64\n",
              "Occupation                    int64\n",
              "Annual_Income               float64\n",
              "Monthly_Inhand_Salary       float64\n",
              "Num_Bank_Accounts             int64\n",
              "Num_Credit_Card               int64\n",
              "Interest_Rate                 int64\n",
              "Num_of_Loan                   int64\n",
              "Delay_from_due_date           int64\n",
              "Num_of_Delayed_Payment        int64\n",
              "Changed_Credit_Limit        float64\n",
              "Num_Credit_Inquiries          int64\n",
              "Credit_Mix                    int64\n",
              "Outstanding_Debt            float64\n",
              "Credit_Utilization_Ratio    float64\n",
              "Credit_History_Age            int64\n",
              "Payment_of_Min_Amount         int64\n",
              "Total_EMI_per_month         float64\n",
              "Amount_invested_monthly     float64\n",
              "Payment_Behaviour             int64\n",
              "Monthly_Balance             float64\n",
              "Credit_Score                  int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dtypes attributes shows the types of data in each column of the data set. This allows us to understand how the data can be manipulated. From this data, we understand that all of the data are numerical data consisting of either integers or float values"
      ],
      "metadata": {
        "id": "JPhHFJH3qKrc"
      },
      "id": "JPhHFJH3qKrc"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe()"
      ],
      "metadata": {
        "id": "lZ0lNJz6p2L_"
      },
      "id": "lZ0lNJz6p2L_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above summary statistic, we can see that each column has 2100 data values which is the same as the number of rows. this explains to us that all the columns has been filled with data and there is no NaN values in the columns of the data set.\n",
        "\n",
        "From the min and max value of Credit_Score being 1 and 3 respectively we know that the data in this column is between 1 to 3. But, from the above dtypes attributes, we identified Credit_Score to have int64 data type in its columns. So, from this we know that the data values in the Credit_Score column is either 1, 2 or 3.\n",
        "\n",
        "From the median age which is 33 and the mean age which is 33.197143, which are close to each other to an age of 33, indicates to us that the average age of data collected from the respondents are at 33, where they are mostly working adults that have high financial commitment"
      ],
      "metadata": {
        "id": "AnNUZfpaqP8R"
      },
      "id": "AnNUZfpaqP8R"
    },
    {
      "cell_type": "markdown",
      "id": "511d03af",
      "metadata": {
        "id": "511d03af"
      },
      "source": [
        "# Supervised learning\n",
        "\n",
        "## What exactly is Supervised learning?\n",
        "\n",
        "- Supervised learning is defined as the process of training an algorithm on labelled data.\n",
        "\n",
        "## What is the notion of labelled data?\n",
        "\n",
        "- Labelled data means the data has already been sorted or categorised. Building a model that can predict the category or output of brand-new, unforeseen data based on the input or properties of the data is the aim of supervised learning.\n",
        "\n",
        "## What are the training and test datasets?\n",
        "\n",
        "- The training dataset refers to the portion of the dataset used in training the ML model. The training dataset is used to optimize the model parameters in order for the model to make accurate predictions on new data.\n",
        "\n",
        "- The test dataset refers to the portion of the dataset used in testing the ML model. The testing dataset is used to evaluate the performance of the already trained model. This dataset is separate and different from the training dataset and is not used to train the model. It is, however, used to test the model's ability to generalize to new data.\n",
        "\n",
        "### Sources:\n",
        "- https://www.ibm.com/topics/supervised-learning\n",
        "- https://www.techtarget.com/searchenterpriseai/definition/supervised-learning\n",
        "- https://www.analyticssteps.com/blogs/binary-and-multiclass-classification-machine-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________________________________________________________________________________________________________________________________________________________________________"
      ],
      "metadata": {
        "id": "8oK7prQiqy16"
      },
      "id": "8oK7prQiqy16"
    },
    {
      "cell_type": "markdown",
      "id": "0a8ff80d",
      "metadata": {
        "id": "0a8ff80d"
      },
      "source": [
        "### Separating the Features and the Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10347f8",
      "metadata": {
        "id": "c10347f8"
      },
      "outputs": [],
      "source": [
        "# selecting all rows, and selected relevant columns\n",
        "features = dataset.iloc[:,[4,5,6,7,8,9,10,11,12,13,14,15,17,18,22]].values\n",
        "\n",
        "# selecting all the rows and the last column only\n",
        "label = dataset.iloc[:,-1].values\n",
        "\n",
        "# .values() converts the selected column into a NumPy array because many ML libaries require the input format to be NumPy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1945fc1b",
      "metadata": {
        "id": "1945fc1b"
      },
      "source": [
        "### Splitting the data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8310d21",
      "metadata": {
        "id": "d8310d21"
      },
      "outputs": [],
      "source": [
        "# 'train_test_split' function is used to split a dataset into the training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, label_train, label_test = train_test_split(features, label, test_size = 0.21, random_state = 0)\n",
        "\n",
        "# test_size = 0.21 means that 21% of the data is used for testing\n",
        "# random_state sets the random seed for the split.\n",
        "# This ensures that the split is reproducible, meaning that if the code is run again with the same seed, the split will be the same."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the training and testing dataset\n",
        "\n",
        "We split our data into training and test sets to measure and analyze the performance of our machine learning models. The training data set which will be used to train the model learns the patterns and relationship of features and labels, while the test set estimates how well the model will generalize to new, unseen data. The splitting of training and testing data set helps in evaluating the performance for a model on a new data.\n",
        "\n",
        "It is also important in preventing overfitting of model which indicates that the machine learning model has fitted the data set used in training too perfect and well until it could not identify the pattern of a new data. With training and testing data set, we can train the model using the training set and fine tune the model which was created using the training data set with the testing data set by evaluating the performance of the model and tuning the hyperparameters of the model. This also ensures that the model can generalise successfully to fresh data.\n",
        "\n",
        "A training-test split of 80|20 to 70|30, is generally considered as a good split between testing and training dataset as there is sud=fficient data for training model and checking the performance of the model"
      ],
      "metadata": {
        "id": "GpEnpVhJr3lj"
      },
      "id": "GpEnpVhJr3lj"
    },
    {
      "cell_type": "markdown",
      "id": "4cc5fd4f",
      "metadata": {
        "id": "4cc5fd4f"
      },
      "source": [
        "# Classification\n",
        "\n",
        "### Explaining the difference between binary and multi-class classification\n",
        "Both binary and multi-class classification are common types of supervised ML tasks that involve the process of assigning a label or category to a given input. However, there are differences between binary and multi-class classification.\n",
        "\n",
        "Binary classification involves classifying given data into one of two classes (or you could say it assigns one of two possible labels to an input). For example, determining whether an email is spam or not is a binary classification problem, as there are only two possible outcomes.\n",
        "\n",
        "Multi-class classification involves classifying elements into different classes (or you could say it assigns one of three or more possible labels to an input). For example, determining the colour of a ball from an image of it is a multi-class classification problem, as there are multiple possible outcomes (e.g. blue, green, red).\n",
        "\n",
        "The main difference between binary and multi-class classification is the number of possible outcomes. In binary classification, there are only 2 possible outcomes, while in multi-class classification, there are more than 2. Multi-class classification, however, doesn't limit itself to any number of classes (unlike binary classification)\n",
        "\n",
        "\n",
        "### Sources:\n",
        "- https://www.analyticssteps.com/blogs/binary-and-multiclass-classification-machine-learning\n",
        "- https://vitalflux.com/difference-binary-multi-class-multi-label-classification/#:~:text=So%2C%20what's%20the%20difference%20between,from%20more%20than%20two%20classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Describing what I understand from the need to normalise data\n",
        "Data normalisation is an important step in the pre-processing stage to prepare data for analysis or machine learning models.\n",
        "\n",
        "The goal is to standardise the numeric columns in the datatset to a common scale because some variables may dominate others if they are on different scales. This reduces the computational complexity of the models, therefore making the optimization process slower and less accurate. Once data is normalized, the optimization process becomes more efficient.\n",
        "\n",
        "Another reason is to improve the accuracy and stability of the models. Many ML algorithms assume that the variables are normally distributed with mean = 0 and its standard deviation = 1. By normalizing data, the variables are made to conform to this assumption, therefore improving the overall performance of the model.\n",
        "\n",
        "### Sources:\n",
        "- https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4680035",
      "metadata": {
        "id": "e4680035"
      },
      "source": [
        "# Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48fe9908",
      "metadata": {
        "id": "48fe9908"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# using StandardScaler function to scale data properly\n",
        "sc = StandardScaler()\n",
        "\n",
        "# scaling the training dataset using fit_transform function. The dataset is scaled to have mean = 0 and a certain standard deviation\n",
        "features_train = sc.fit_transform(features_train)\n",
        "\n",
        "# transform() function applies the mean and standard deviation values to the testing dataset\n",
        "features_test = sc.transform(features_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4cde886",
      "metadata": {
        "id": "e4cde886"
      },
      "source": [
        "### Describing SVM (in relation to Linear Regression)\n",
        "\n",
        "Support Vector Machine (abbreviated as SVM) is a type of supervised learning algorithm used for both classification and regression tasks. The goal of SVM is to find the best hyperplane (decision boundaries used to predict the continuous output) that separates the data points into different classes. The criteria for this is that it maximizes the margin between the closest points from each class.\n",
        "\n",
        "The goal of linear regression is to find the line that best fits the data points. The line of best fit is chosen following the criteria that the sum of the squared distances between the actual data points and the predicted values on the line is minimized.\n",
        "\n",
        "Both of these algortihms attempt to find a linear relationship between the input features and the output variable. SVM however, is more flexible than linear regression as it can handle both linear and non-linear relationships between the input features and the output variable. While both of these algorithms aim to find a relationship between the input features and the output variable, they have different goals and approaches to how they achieve this. Linear regression tries to minimize the sum of squared distances between the predicted and actual values while SVM tries to maximize the margin between the classes.\n",
        "\n",
        "### Sources:\n",
        "- https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0\n",
        "- https://www.analyticssteps.com/blogs/how-does-support-vector-machine-algorithm-works-machine-learning\n",
        "\n",
        "### What is the kernel in SVM?\n",
        "In SVM, the kernel is defined as a set of mathematical functions. Its purpose is to transform the input data into a higher-dimensional space, where it can more easily be separated into different classes. The kernel function calculates the dot product between pairs of data points in the original input space and maps them to a higher-dimensional feature space.\n",
        "\n",
        "Different SVMs use different types of kernel functions. Choosing the correct kernel is important as it determines the shape of the decision boundary or hyperplane that separates the data points. Examples of different types of kernels include:\n",
        "\n",
        "#### - Linear\n",
        "This calculates the dot product in between the input feature vectors in the original space. This kernel is useful for linearly separable data.\n",
        "\n",
        "#### - Polynomial\n",
        "This uses a polynomial function to map the input data to a higher-dimensional feature space. The polynomial function can handle non-linearly separable data.\n",
        "\n",
        "#### - RBF\n",
        "This kernel maps the input data to an infinite-dimensional feature space using a Gaussian function. It is commonly used for non-linearly separable data.\n",
        "\n",
        "#### - Sigmoid\n",
        "Similar to the polynomial kernel, but this kernel uses a hyperbolic tangent function instead of a polynomial function.\n",
        "\n",
        "The choice of kernel being used can impact the performance of the SVM greatly, so it is important to pick the appropriate kernel based on the nature of the data and problem at hand.\n",
        "\n",
        "### Sources:\n",
        "- https://data-flair.training/blogs/svm-kernel-functions/\n",
        "- https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27020253",
      "metadata": {
        "id": "27020253"
      },
      "source": [
        "### Building the model from training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b9361e",
      "metadata": {
        "id": "e2b9361e",
        "outputId": "26b4ad2a-8ec8-40f0-dc02-8d48c9dabec4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# here we initialize a SVC object called \"classifier\" that uses the kernel \"rbf\"\n",
        "classifier = SVC(kernel = 'rbf')\n",
        "\n",
        "# training the model\n",
        "# features_train is the input feature data & label_train is the corresponding output or targeted data\n",
        "classifier.fit(features_train, label_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9072e396",
      "metadata": {
        "id": "9072e396"
      },
      "source": [
        "### Prediction for credit score label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81f41b4",
      "metadata": {
        "id": "c81f41b4",
        "outputId": "c1ca89f6-fe17-4c2b-ea9b-9699ce1a2ee3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 3, 1, 2, 2, 2, 3, 2, 1, 2, 2, 1, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3,\n",
              "       2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 3, 1, 1,\n",
              "       3, 2, 2, 2, 2, 1, 3, 3, 3, 1, 1, 3, 1, 1, 3, 2, 2, 1, 2, 2, 1, 2,\n",
              "       2, 3, 3, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, 2, 2, 1,\n",
              "       3, 2, 1, 2, 2, 2, 1, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 1, 2, 3, 2, 2,\n",
              "       1, 1, 3, 3, 3, 3, 1, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2,\n",
              "       1, 2, 2, 2, 2, 2, 3, 1, 1, 1, 3, 1, 3, 2, 2, 2, 1, 2, 1, 2, 2, 1,\n",
              "       3, 3, 2, 1, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 2, 1, 2,\n",
              "       2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 3, 2,\n",
              "       2, 3, 1, 1, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2,\n",
              "       1, 2, 2, 1, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 2, 1,\n",
              "       2, 1, 2, 2, 3, 2, 2, 2, 2, 1, 3, 3, 2, 1, 2, 1, 3, 2, 2, 2, 2, 2,\n",
              "       2, 2, 1, 2, 1, 3, 2, 2, 1, 3, 2, 3, 2, 2, 2, 1, 1, 2, 1, 2, 3, 2,\n",
              "       1, 2, 3, 2, 1, 2, 3, 3, 1, 3, 3, 2, 1, 1, 2, 3, 1, 1, 2, 2, 3, 3,\n",
              "       2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 3, 3, 1, 2, 2, 3, 2,\n",
              "       3, 2, 3, 2, 3, 2, 1, 1, 2, 1, 3, 1, 2, 2, 3, 2, 3, 1, 3, 2, 2, 3,\n",
              "       3, 2, 1, 2, 1, 1, 3, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2,\n",
              "       2, 1, 2, 2, 2, 2, 3, 3, 1, 2, 1, 3, 1, 2, 2, 3, 2, 1, 3, 2, 2, 3,\n",
              "       3, 2, 3, 2, 1, 2, 2, 1, 2, 3, 2, 2, 3, 3, 1, 2, 2, 2, 3, 2, 2, 3,\n",
              "       2])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# predicting & displaying the test set results\n",
        "label_prediction = classifier.predict(features_test)\n",
        "label_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b980c51f",
      "metadata": {
        "id": "b980c51f"
      },
      "source": [
        "### Displaying confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e28453",
      "metadata": {
        "id": "a2e28453",
        "outputId": "5fa8bd1f-8c25-4377-d857-7a3d2f88fa95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 65,  49,  12],\n",
              "       [ 27, 180,  34],\n",
              "       [  0,  26,  48]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing confusion matrix library\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# making and displaying the confusion matrix\n",
        "cmatrix = confusion_matrix(label_test, label_prediction)\n",
        "cmatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7018daef",
      "metadata": {
        "id": "7018daef"
      },
      "source": [
        "#### Explanation for confusion matrix\n",
        "\n",
        "The output that can be observed above is a 3x3 confusion matrix. The instances in a predicted class is represented by rows of the matrix. The instances in an actual class is represented by columns of the matrix. The numbers in the matrix represent the counts of instances that fall into each of these categories.\n",
        "\n",
        "More specifically, the confusion matrix can be interpreted as follows:\n",
        "\n",
        "- The first row represents the instances that were actually in the first class. Out of these instances, 65 were correctly classified as belonging to the first class, 49 were incorrectly classified as belonging to the second class, and 12 were incorrectly classified as belonging to the third class.\n",
        "\n",
        "- The second row represents the instances that were actually in the second class. Out of these instances, 27 were incorrectly classified as belonging to the first class, 180 were correctly classified as belonging to the second class, and 34 were incorrectly classified as belonging to the third class.\n",
        "\n",
        "- The third row represents the instances that were actually in the third class. Out of these instances, none were incorrectly classified as belonging to the first or second class, but 26 were incorrectly classified as belonging to the third class, and 48 were correctly classified as belonging to the third class.\n",
        "\n",
        "Overall, this confusion matrix can be used to calculate various performance metrics for the classification model, such as accuracy, precision, and recall.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb9bea04",
      "metadata": {
        "id": "eb9bea04"
      },
      "source": [
        "### Quadratic Weighted Kappa (QWK)\n",
        "\n",
        "Quadratic Weighted Kappa (QWK) is a statistical measure that is commonly used to evaluate the agreement between two raters, for example two different ML algorithms. The QWK ranges from -1 to 1. -1 indicates complete disagreement, 0 indicates the level of agreement expected by chance, and 1 indicates complete agreement.\n",
        "\n",
        "We calculate the QWK by comparing the observed agreement between the raters with the expected agreement.\n",
        "- The observed agreement is the proportion of cases where the raters agree on the rating\n",
        "- The expected agreement is the proportionn of cases where the raters would be expected to agree by chance, based on the distribution of ratings.\n",
        "\n",
        "QWK is very useful when evaluating the performance of ML algorithms. This is because it can capture the agreement between the algorithm's predictions and the actual labels of the data, even when there are multiple possible categories. For example, in a multi-class classification task, QWK can be used to evaluate the agreement between the predicted class and the actual class, taking into account the degree of disagreement between the different classes.\n",
        "\n",
        "### Sources:\n",
        "- https://www.kaggle.com/code/carlolepelaars/understanding-the-metric-quadratic-weighted-kappa\n",
        "- https://verosssr.com/cacefc41e57e499a9f4c8cdce878df5d\n",
        "- https://medium.com/x8-the-ai-community/kappa-coefficient-for-dummies-84d98b6f13ee"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e561ac2f",
      "metadata": {
        "id": "e561ac2f"
      },
      "source": [
        "### Using the sklearn.metrics library to code and obtain the QWK score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "935162ba",
      "metadata": {
        "id": "935162ba",
        "outputId": "14d716fe-4d3b-4639-8a0a-18d21a0bb8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QWK score: 0.5239\n"
          ]
        }
      ],
      "source": [
        "# importing cohen kappa score library\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# obtain the true labels for the test set\n",
        "label_true = label_test\n",
        "\n",
        "# compute the QWK score for the predicted and true labels\n",
        "qwk_score = cohen_kappa_score(label_true, label_prediction, weights = 'quadratic')\n",
        "print(\"QWK score: {:.4f}\".format(qwk_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c928d2ea",
      "metadata": {
        "id": "c928d2ea"
      },
      "source": [
        "#### Explanation for the QWK score:\n",
        "\n",
        "The QWK score of 0.5239 indicates a moderate level of agreement between two raters or between the predicted and true values. This means that my model's predictions are better than random guessing, but there is still room for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a383c393",
      "metadata": {
        "id": "a383c393"
      },
      "source": [
        "# Kaggle submission – Credit Score Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf04bd3",
      "metadata": {
        "id": "7cf04bd3",
        "outputId": "b866df2c-275a-409e-a9c6-4ab80e1d54ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 1 2 2 2 3 3 2 2 2 2 2 3 1 1 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 3 1 3 1 1 1\n",
            " 1 2 3 3 2 2 1 1 2 2 2 2 1 2 2 2 3 2 2 2 3 2 1 2 3 3 1 2 2 1 2 2 3 3 2 2 1\n",
            " 2 2 2 2 2 2 2 3 1 1 1 1 1 2 2 2 2 2 3 1 1 2 2 2 2 3 2 3 3 2 2 2 1 2 2 3 2\n",
            " 1 2 1 3 2 1 2 3 2 3 2 2 2 2 1 3 3 1 1 2 1 2 2 1 1 1 2 1 1 2 2 3 3 2 1 2 1\n",
            " 2 3 2 1 1 2 2 2 2 1 2 1 2 3 2 1 1 1 1 3 2 2 3 2 3 2 1 2 2 2 1 2 1 2 2 1 2\n",
            " 1 2 2 1 2 3 2 1 1 3 3 2 3 2 3 1 2 2 2 2 1 2 2 2 2 3 2 1 2 3 1 1 2 2 2 3 2\n",
            " 3 2 2 3 2 3 2 2 1 3 1 2 2 2 2 1 2 2 1 1 1 1 2 1 2 2 1 1 2 2 3 2 1 3 2 3 1\n",
            " 2 2 2 3 3 2 3 1 3 3 2 3 2 1 1 2 1 2 2 3 2 1 1 3 2 1 3 2 2 3 3 2 2 3 2 2 2\n",
            " 2 3 2 1 2 2 3 2 2 1 2 1 1 3 3 3 3 3 2 2 1 3 2 2 2 2 2 2 2 1 3 2 2 2 3 2 1\n",
            " 1 1 2 2 2 2 2 1 2 2 2 3 2 3 2 3 3 2 2 1 1 3 1 2 2 1 2 3 2 1 2 2 2 2 2 1 2\n",
            " 1 1 2 1 2 2 2 1 1 2 3 2 3 2 2 3 2 1 3 2 3 2 2 1 1 1 2 2 2 2 1 1 2 1 2 1 2\n",
            " 3 2 3 3 2 3 2 2 1 2 2 2 2 2 1 1 2 1 2 1 3 1 2 2 3 3 3 2 3 3 3 1 1 2 2 1 2\n",
            " 2 1 3 1 2 2 2 2 1 2 3 2 2 2 2 2 1 3 3 2 3 1 2 3 3 2 3 2 2 3 3 2 2 1 3 3 1\n",
            " 2 2 1 2 2 1 2 2 2 2 2 3 3 2 2 2 1 2 1 3 1 3 1 2 3 2 2 2 2 2 2 2 2 2 2 2 3\n",
            " 1 2 1 1 1 2 3 1 1 2 3 2 2 1 2 3 2 1 1 2 1 3 1 2 2 2 2 2 2 1 2 2 3 3 2 2 2\n",
            " 1 2 3 3 2 2 2 2 1 2 2 1 2 1 2 2 2 2 3 2 2 2 2 1 2 2 2 2 1 2 3 3 2 2 2 3 1\n",
            " 2 2 2 3 3 2 2 2 2 3 1 1 2 2 3 2 3 2 3 2 3 2 2 2 2 2 1 2 2 1 2 2 2 1 1 1 2\n",
            " 2 3 3 3 2 1 2 1 2 3 2 3 1 2 1 1 1 2 2 1 2 2 2 2 3 3 3 1 1 3 3 2 2 2 2 2 2\n",
            " 2 1 1 2 2 1 2 3 2 2 1 1 3 2 1 1 3 3 2 1 3 1 2 1 1 3 2 3 1 2 1 3 2 2 2 2 3\n",
            " 3 2 2 2 3 1 2 2 2 2 1 1 3 2 1 2 3 1 2 2 1 2 2 1 1 2 2 3 3 1 3 2 2 2 1 2 3\n",
            " 1 2 1 1 2 2 2 2 1 2 2 2 2 2 3 2 2 2 2 2 2 2 3 1 2 2 2 2 2 2 2 2 2 1 1 2 2\n",
            " 2 1 2 2 2 1 1 1 2 1 2 1 1 2 2 2 2 2 2 2 2 3 2 2 3 2 3 1 2 2 2 1 3 1 1 1 2\n",
            " 3 2 3 2 3 1 1 1 3 3 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 1 2 2 3 2 3 2 1\n",
            " 2 3 2 2 1 1 2 3 1 2 1 1 2 3 1 3 1 2 1 2 1 1 2 1 2 2 1 2 2 1 2 2 1 1 2 1 1\n",
            " 1 2 1 3 2 3 2 2 2 2 2 2] 900\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# we are using the same model that we've created earlier but using a different dataset\n",
        "dataset2 = pd.read_csv('Credit-Scores-Submission.csv')\n",
        "\n",
        "# separating features and label\n",
        "feature = dataset2.iloc[:,[4,5,6,7,8,9,10,11,12,13,14,15,17,18,22]].values\n",
        "\n",
        "# scaling\n",
        "feature = sc.transform(feature)\n",
        "\n",
        "# predicting credit scores for the test set\n",
        "label_pred = classifier.predict(feature)\n",
        "print(label_pred, len(label_pred))\n",
        "\n",
        "# for this part, two files were utilised: one for input, one for output\n",
        "with open('32845650-WongJunWei-v1.csv', 'r') as csv_file, open('32845650-Wong_Jun_Wei-v1.csv', 'w', newline = '') as output_file:\n",
        "    csvreader = csv.reader(csv_file)\n",
        "    csvwriter = csv.writer(output_file)\n",
        "\n",
        "    # iterating over rows of the input csv file, then updates the credit score, and lastly writes the updated rows to the output csv file\n",
        "    for i, row in enumerate(csvreader):\n",
        "        # for the first row (which is the header), write it to the output CSV file\n",
        "        if i == 0:\n",
        "            csvwriter.writerow(row)\n",
        "        else:\n",
        "            # row[1] refers to the credit score, therefore this updates the credit score of that row\n",
        "            row[1] = label_pred[i - 1]\n",
        "            # write the updated row to the output CSV file\n",
        "            csvwriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63ab4fad",
      "metadata": {
        "id": "63ab4fad"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "From this project, I have learnt that a predictive model of Support Vector Machine (SVM) is a strong library that can be used in a wide range of problems involving predicting values. I have also learnt that SVM can not only be used for classification purposes (as attempted in the project), but also can be used to solve for regression problems. I have also learnt the importance of normalizing or scaling the data in the pre-processing stage before processing a predictive model. I have also learnt about a the importance of using appropriate hyperparameters and tuning the hyperparameters in a SVM model to get a good predictive model that has an optimal performance. I also learnt how to evaluate the model using various metrics and validation techniques to ensure that it is accurate and generalizes well to new data. Overall, I hope the knowledge gained from this project will help me in the field of study in which I pursue in the future."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5SDDx3duLOo"
      },
      "id": "G5SDDx3duLOo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}